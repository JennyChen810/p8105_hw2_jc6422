p8105_hw2_jc6422
================
Jianing Chen
2024-10-03

## Problem 1

Load the necessary packages.

``` r
library(tidyverse)
library(readxl)
```

Load and clean the data from
`NYC_Transit_Subway_Entrance_And_Exit_Data.csv`.

``` r
nyc_df = 
  read_csv(
    "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) |> 
  janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

This dataset provides information about entrances and exits for subway
stations in the New York city. The variables in this dataset are line,
station_name,station_latitude, station_longitude, route1, route2,
route3, route4, route5, entry,vending,entrance_type, ada. To clean the
data, I use `janitor::clean_names()` to convert the column names to
lowercase and replaced the special characters and spaces with
underscores. The `entry` variable was transformed from a character like
`YES` or `No` to a logical variable like `TRUE` or `FALSE`. As part of
data import, we specify that `Route` columns 8-11 should be character
for consistency with 1-7. The cleaned data has a dimension of 1868 and
20 columns, which retains the necessary variables for analyzing this
data. These data are tidy since each variable forms a column, each
observation forms a row, and each value is uniquely located at the
intersection of a row and a column.

Calculate the number of distinct stations. Selects station name and
line, and then uses `distinct()` to obtain all unique combinations.

``` r
nyc_df |> 
  select(station_name, line) |> 
  distinct()
## # A tibble: 465 × 2
##    station_name             line    
##    <chr>                    <chr>   
##  1 25th St                  4 Avenue
##  2 36th St                  4 Avenue
##  3 45th St                  4 Avenue
##  4 53rd St                  4 Avenue
##  5 59th St                  4 Avenue
##  6 77th St                  4 Avenue
##  7 86th St                  4 Avenue
##  8 95th St                  4 Avenue
##  9 9th St                   4 Avenue
## 10 Atlantic Av-Barclays Ctr 4 Avenue
## # ℹ 455 more rows
```

There are 465 distinct stations.

Calculate the number of stations are ADA compliant.

``` r
nyc_df |> 
  filter(ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
## # A tibble: 84 × 2
##    station_name                   line           
##    <chr>                          <chr>          
##  1 Atlantic Av-Barclays Ctr       4 Avenue       
##  2 DeKalb Av                      4 Avenue       
##  3 Pacific St                     4 Avenue       
##  4 Grand Central                  42nd St Shuttle
##  5 34th St                        6 Avenue       
##  6 47-50th Sts Rockefeller Center 6 Avenue       
##  7 Church Av                      6 Avenue       
##  8 21st St                        63rd Street    
##  9 Lexington Av                   63rd Street    
## 10 Roosevelt Island               63rd Street    
## # ℹ 74 more rows
```

There are 84 stations are ADA compliant.

Calculate the proportion of stations entrances/exist without vending
allow entrance.

``` r
prop_no_vending = nyc_df |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

The proportion of station entrances/exits without vending is 0.3770492.

Reformat data

Convert `route` from wide to long format. Then, we can use tools from
previous parts of the question (filtering to focus on the A train, and
on ADA compliance; selecting and using `distinct` to obtain dataframes
with the required stations in rows).

``` r
A_train = nyc_df |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()

ADA_compliant = nyc_df |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

There are 60 distinct stations served the A train.

There are 17 distinct A stations that are ADA compliant.

## Problem 2

Read and clean the Mr.Trash Wheel sheet.

First, clean each sheet in the dataset with `read_excel`, filters out
non-data rows, rounds sports balls to the nearest integer, and adds a
trash_wheel column for identification.

``` r
library(readxl)
clean_trash_wheel =
    read_excel("./data/202409_Trash_Wheel_Collection_Data.xlsx",sheet = "Mr. Trash Wheel") %>%
    janitor::clean_names() %>% 
    select(-x15, -x16)%>%
    filter(!is.na(dumpster)) %>% 
    mutate(sports_balls = as.integer(round(sports_balls)),
           year = as.numeric(year))
```

Mr. trash wheel sheet has 651 rows and 14 columns. The variables are
dumpster, month, year, date, weight_tons, volume_cubic_yards,
plastic_bottles, polystyrene, cigarette_butts, glass_bottles,
plastic_bags, wrappers, sports_balls, homes_powered.

Read and clean the data for Professor Trash Wheel.

``` r
clean_professor =
    read_excel("./data/202409_Trash_Wheel_Collection_Data.xlsx", sheet = "Professor Trash Wheel") %>%
    janitor::clean_names() %>% 
    filter(!is.na(dumpster)) %>%
    mutate(year = as.numeric(year))
```

Professor trash wheel sheet has 119 rows and 13 columns. The variables
are dumpster, month, year, date, weight_tons, volume_cubic_yards,
plastic_bottles, polystyrene, cigarette_butts, glass_bottles,
plastic_bags, wrappers, homes_powered.

Read and clean the data for Gwynnda Trash Wheel

``` r
clean_gwynnda =
    read_excel("./data/202409_Trash_Wheel_Collection_Data.xlsx", sheet = "Gwynnda Trash Wheel") %>%
    janitor::clean_names() %>% 
    filter(!is.na(dumpster)) %>%
    mutate(year = as.numeric(year))

print(clean_gwynnda)
## # A tibble: 263 × 12
##    dumpster month   year date                weight_tons volume_cubic_yards
##       <dbl> <chr>  <dbl> <dttm>                    <dbl>              <dbl>
##  1        1 July    2021 2021-07-03 00:00:00        0.93                 15
##  2        2 July    2021 2021-07-07 00:00:00        2.26                 15
##  3        3 July    2021 2021-07-07 00:00:00        1.62                 15
##  4        4 July    2021 2021-07-16 00:00:00        1.76                 15
##  5        5 July    2021 2021-07-30 00:00:00        1.53                 15
##  6        6 August  2021 2021-08-11 00:00:00        2.06                 15
##  7        7 August  2021 2021-08-14 00:00:00        1.9                  15
##  8        8 August  2021 2021-08-16 00:00:00        2.16                 15
##  9        9 August  2021 2021-08-16 00:00:00        2.6                  15
## 10       10 August  2021 2021-08-17 00:00:00        3.21                 15
## # ℹ 253 more rows
## # ℹ 6 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
## #   cigarette_butts <dbl>, plastic_bags <dbl>, wrappers <dbl>,
## #   homes_powered <dbl>
```

Gwynnda trash wheel sheet has 263 rows and 12 columns. The variables are
dumpster, month, year, date, weight_tons, volume_cubic_yards,
plastic_bottles, polystyrene, cigarette_butts, plastic_bags, wrappers,
homes_powered.

Then combine these sheets by `bind_rows`.

``` r
clean_trash_wheel = clean_trash_wheel %>% 
  mutate(trash_wheel = "Mr. Trash Wheel")
clean_professor = clean_professor %>% 
  mutate(trash_wheel = "Professor Trash Wheel")
clean_gwynnda = clean_gwynnda %>% 
  mutate(trash_wheel = "Gwynnda")

combined_trash_data = bind_rows(clean_trash_wheel, clean_professor, clean_gwynnda)
```

The new dataset has 1033 observations and 15 columns. The key variables:
dumpster, month, year, date, weight_tons, volume_cubic_yards,
plastic_bottles, polystyrene, cigarette_butts, glass_bottles,
plastic_bags, wrappers, sports_balls, homes_powered, trash_wheel.

Calculate the total weight of trash collected by Professor Trash Wheel.

``` r
professor_trash_total_weight = combined_trash_data %>%
  filter(trash_wheel == "Professor Trash Wheel") %>%
  summarize(total_weight_tons = sum(weight_tons, na.rm = TRUE))
```

The total trash weight collected by Professor Trash Wheel is 0 tons.

Calculate the total number of cigarette butts collected by Gwynnda in
June of 2022.

``` r
gwynnda_cigarette_butts = combined_trash_data %>%
  filter(trash_wheel == "Gwynnda", format(date, "%Y-%m") == "2022-06") %>%
  summarize(total_cigarette_butts = sum(cigarette_butts, na.rm = TRUE))
```

- The total number of cigarette butts collected by Gwynnda in June of
  2022 is 0.

## Problem 3

load and clean the data

``` r
bakers =
  read_csv("./data/bakers.csv",na = c("NA", "N/A", "UNKNOWN", "Unknown", ""))%>%
  janitor::clean_names() %>% 
  mutate(baker_name = str_trim(baker_name)) %>%
  select(baker_name, everything())

bakes = 
  read_csv("./data/bakes.csv",na = c("NA", "N/A", "UNKNOWN", "Unknown", "")) %>%
  janitor::clean_names() %>% 
  mutate(baker = str_trim(baker)) %>%
  select(baker, everything())

results = read_csv("./data/results.csv",skip=2) %>%
  janitor::clean_names() %>% 
  mutate(
    baker = str_trim(baker),
    result = str_to_upper(result)) %>%
  select(baker, everything())
```

Imput each dataset `bakers.csv`, `bakes.csv`, and `results.csv` then use
the `read_csv()` function reads the CSV files. Special values like “NA”,
“N/A”, “UNKNOWN”, “Unknown”, and empty strings are treated as missing
(NA). I use `janitor::clean_names()` to each dataset to standardize the
columns. Then I use `mutate()`function to remove any leading or trailing
whitespace from the `baker_name` column using `str_trim()`. Reorders
columns so that `baker_name` is the first column.

Use `anti_join()` function to check for completeness and correctness
across datasets.

``` r
anti_join(bakes, bakers)
## # A tibble: 0 × 5
## # ℹ 5 variables: baker <chr>, series <dbl>, episode <dbl>,
## #   signature_bake <chr>, show_stopper <chr>
anti_join(bakes, results)
## # A tibble: 8 × 5
##   baker    series episode signature_bake                            show_stopper
##   <chr>     <dbl>   <dbl> <chr>                                     <chr>       
## 1 "\"Jo\""      2       1 Chocolate Orange CupcakesOrange and Card… Chocolate a…
## 2 "\"Jo\""      2       2 Caramelised Onion, Gruyere and Thyme Qui… Raspberry a…
## 3 "\"Jo\""      2       3 Stromboli flavored with Mozzarella, Ham,… <NA>        
## 4 "\"Jo\""      2       4 Lavender Biscuits                         Blueberry M…
## 5 "\"Jo\""      2       5 Salmon and Asparagus Pie                  Apple and R…
## 6 "\"Jo\""      2       6 Rum and Raisin Baked Cheesecake           Limoncello …
## 7 "\"Jo\""      2       7 Raspberry & Strawberry Mousse Cake        Pain Aux Ra…
## 8 "\"Jo\""      2       8 Raspberry and Blueberry Mille Feuille     Mini Victor…
```

The result of `anti_join(results,their_bake)` shows cannot find the
corresponding results in all the episodes jo attended. But Jo might be
Joanne who is the winner of series 2. Therefore, I changed the first
name Joanne to Jo in the result dataframe.

merge the data bakers and bakes on series and baker.

``` r
merged_data = bakes %>%
  left_join(bakers, by = c("series" = "series", "baker" = "baker_name"))

final_data = merged_data %>%
  left_join(results, by = c("series" = "series", "episode" = "episode", "baker" = "baker"))

print(final_data)
## # A tibble: 548 × 10
##    baker   series episode signature_bake show_stopper baker_age baker_occupation
##    <chr>    <dbl>   <dbl> <chr>          <chr>            <dbl> <chr>           
##  1 Annetha      1       1 "Light Jamaic… Red, White …        NA <NA>            
##  2 David        1       1 "Chocolate Or… Black Fores…        NA <NA>            
##  3 Edd          1       1 "Caramel Cinn… <NA>                NA <NA>            
##  4 Jasmin…      1       1 "Fresh Mango … <NA>                NA <NA>            
##  5 Jonath…      1       1 "Carrot Cake … Three Tiere…        NA <NA>            
##  6 Lea          1       1 "Cranberry an… Raspberries…        NA <NA>            
##  7 Louise       1       1 "Carrot and O… Never Fail …        NA <NA>            
##  8 Mark         1       1 "Sticky Marma… Heart-shape…        NA <NA>            
##  9 Miranda      1       1 "Triple Layer… Three Tiere…        NA <NA>            
## 10 Ruth         1       1 "Three Tiered… Classic Cho…        NA <NA>            
## # ℹ 538 more rows
## # ℹ 3 more variables: hometown <chr>, technical <dbl>, result <chr>
write_csv(final_data, "final_data.csv")
```

Create a table to show the Star Bakers and Winners.

``` r
star_bakers = final_data %>%
  filter(result %in% c('STAR BAKER', 'WINNER')) %>%
  select(series, episode, baker, result)

star_bakers_seasons = star_bakers %>%
  filter(series >= 5 & series <= 10)

print(star_bakers_seasons)
## # A tibble: 40 × 4
##    series episode baker   result    
##     <dbl>   <dbl> <chr>   <chr>     
##  1      5       1 Nancy   STAR BAKER
##  2      5       2 Richard STAR BAKER
##  3      5       3 Luis    STAR BAKER
##  4      5       4 Richard STAR BAKER
##  5      5       5 Kate    STAR BAKER
##  6      5       6 Chetna  STAR BAKER
##  7      5       7 Richard STAR BAKER
##  8      5       8 Richard STAR BAKER
##  9      5       9 Richard STAR BAKER
## 10      5      10 Nancy   WINNER    
## # ℹ 30 more rows
```

In this table, we can observe that the series winner is often start
baker, but even though people win the most star baker titles do not
always win the series. Therefore, the series winner could be someone who
do not have the most star baker titles through the season.

load and clean the `Viewers.csv`.

First load the `Viewers.csv` file and figure out the NA values. Use
`janitor::clean_names()` to standardize the columns. Mutate the season
variable to integer. Then use pivot_longer to organize the data.

``` r
viewers = 
  read_csv("./data/viewers.csv",na = c("NA", ".", "")) %>%
  janitor::clean_names() %>%
  pivot_longer(series_1: series_10,
               names_to = "season",
               names_prefix = "series_",
               values_to = "viewership") %>% 
  mutate(season= as.integer(season))

head(viewers, 10)
## # A tibble: 10 × 3
##    episode season viewership
##      <dbl>  <int>      <dbl>
##  1       1      1       2.24
##  2       1      2       3.1 
##  3       1      3       3.85
##  4       1      4       6.6 
##  5       1      5       8.51
##  6       1      6      11.6 
##  7       1      7      13.6 
##  8       1      8       9.46
##  9       1      9       9.55
## 10       1     10       9.62
```

The average viewership in Season 1 is 2.77.

The average viewership in Season 5 is 10.04.
